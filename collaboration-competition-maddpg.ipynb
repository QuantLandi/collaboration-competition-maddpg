{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collaboration and Competition with MADDPG\n",
    "\n",
    "---\n",
    "### Start the Environment\n",
    "\n",
    "If the code cell below returns an error, please  double-check that you have installed [Unity ML-Agents](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Installation.md) and [NumPy](http://www.numpy.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "import torch\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "from collections import namedtuple, deque\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Before running the code cell below_**, change the `file_name` parameter to match the location of the Unity environment that you downloaded.\n",
    "\n",
    "- **Mac**: `\"path/to/Tennis.app\"`\n",
    "- **Windows** (x86): `\"path/to/Tennis_Windows_x86/Tennis.exe\"`\n",
    "- **Windows** (x86_64): `\"path/to/Tennis_Windows_x86_64/Tennis.exe\"`\n",
    "- **Linux** (x86): `\"path/to/Tennis_Linux/Tennis.x86\"`\n",
    "- **Linux** (x86_64): `\"path/to/Tennis_Linux/Tennis.x86_64\"`\n",
    "- **Linux** (x86, headless): `\"path/to/Tennis_Linux_NoVis/Tennis.x86\"`\n",
    "- **Linux** (x86_64, headless): `\"path/to/Tennis_Linux_NoVis/Tennis.x86_64\"`\n",
    "\n",
    "For instance, if you are using a Mac, then you downloaded `Tennis.app`.  If this file is in the same folder as the notebook, then the line below should appear as follows:\n",
    "```\n",
    "env = UnityEnvironment(file_name=\"Tennis.app\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "env = UnityEnvironment(file_name=\"Tennis_Windows_x86_64/Tennis.exe\")\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set parameters\n",
    "BUFFER_SIZE = 100000  # replay buffer size\n",
    "BATCH_SIZE = 512      # minibatch size\n",
    "GAMMA = 0.99          # discount factor\n",
    "TAU = 0.001           # for soft update of target parameters\n",
    "ACTOR_LR = 0.001      # actor learning rate \n",
    "CRITIC_LR = 0.001     # critic learning rate \n",
    "UPDATE_EVERY = 2      # how often to update the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ornstein-Uhlenbeck noise process by Alexis Cook from Udacity\n",
    "# https://github.com/udacity/deep-reinforcement-learning/tree/master/ddpg-bipedal\n",
    "class OUNoise:\n",
    "    \"\"\"Ornstein-Uhlenbeck process.\"\"\"\n",
    "    def __init__(self, size, seed, mu=0., theta=0.15, sigma=0.2):\n",
    "        \"\"\"Initialize parameters and noise process.\"\"\"\n",
    "        self.mu = mu * np.ones(size)\n",
    "        self.theta = theta\n",
    "        self.sigma = sigma\n",
    "        self.seed = random.seed(seed)\n",
    "        self.size = size\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Reset the internal state (= noise) to mean (mu).\"\"\"\n",
    "        self.state = copy.copy(self.mu)\n",
    "\n",
    "    def sample(self):\n",
    "        \"\"\"Update internal state and return it as a noise sample.\"\"\"\n",
    "        x = self.state\n",
    "        dx = self.theta * (self.mu - x) + self.sigma * np.random.standard_normal(self.size)\n",
    "        self.state = x + dx\n",
    "        return self.state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define actor and critic model architecture\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(24, 400)\n",
    "        self.fc2 = nn.Linear(400+2, 300)\n",
    "        self.fc3 = nn.Linear(300, 1)\n",
    "        \n",
    "    def forward(self, x, action):\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = torch.cat([x, action], 1)\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(24, 200)\n",
    "        self.fc2 = nn.Linear(200, 150)\n",
    "        self.fc3 = nn.Linear(150, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        return torch.tanh(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replay buffer by Timo P. Gros\n",
    "# https://github.com/TimoPGros/\n",
    "class ReplayBuffer:\n",
    "    \n",
    "    def __init__(self, buffer_size, batch_size, seed):\n",
    "        self.batch_size = batch_size\n",
    "        self.memory = deque(maxlen=buffer_size)\n",
    "        self.seed = random.seed(seed)\n",
    "    \n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        if done:\n",
    "            done_value = 1\n",
    "        else:\n",
    "            done_value = 0\n",
    "        self.memory.append([state, action, reward, next_state, done_value])\n",
    "    \n",
    "    def sample(self):\n",
    "        samples = random.sample(self.memory, self.batch_size)\n",
    "        \n",
    "        states = []\n",
    "        actions = []\n",
    "        rewards = []\n",
    "        next_states = []\n",
    "        dones = []\n",
    "        \n",
    "        for sample in samples:\n",
    "            state, action, reward, next_state, done = sample\n",
    "            \n",
    "            states.append(torch.tensor(state).float())\n",
    "            actions.append(torch.tensor(action).float())\n",
    "            rewards.append(reward)\n",
    "            next_states.append(next_state)\n",
    "            dones.append(done)\n",
    "\n",
    "        states = torch.cat(states).float().view(len(samples), -1)\n",
    "        actions = torch.cat(actions).float().view(len(samples), -1)\n",
    "        rewards = torch.tensor(rewards).float()\n",
    "        next_states = torch.tensor(next_states).float()\n",
    "        dones = torch.tensor(dones).float()\n",
    "        \n",
    "        return [states, actions, rewards, next_states, dones]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agent class largely inspired by Alexis Cook from Udacity\n",
    "# https://github.com/udacity/deep-reinforcement-learning/tree/master/ddpg-bipedal\n",
    "class Agent():\n",
    "    def __init__(self, seed):\n",
    "        self.critic_local = Critic()\n",
    "        self.critic_target = Critic()\n",
    "        self.actor_local = Actor()\n",
    "        self.actor_target = Actor()\n",
    "        self.seed = random.seed(seed)\n",
    "        \n",
    "        self.actor_optimizer = optim.Adam(self.actor_local.parameters(), lr=ACTOR_LR)\n",
    "        self.critic_optimizer = optim.Adam(self.critic_local.parameters(), lr=CRITIC_LR)\n",
    "        self.memory = ReplayBuffer(BUFFER_SIZE, BATCH_SIZE, seed)\n",
    "        \n",
    "        self.noise = OUNoise(2, self.seed)\n",
    "        \n",
    "        self.t_step = 0\n",
    "        \n",
    "    def step(self, state, action, reward, next_state, done):\n",
    "        self.memory.add(state, action, reward, next_state, done)\n",
    "        \n",
    "        self.t_step = (self.t_step + 1) % UPDATE_EVERY\n",
    "        if self.t_step == 0:\n",
    "            if (len(self.memory)) > BATCH_SIZE:\n",
    "                samples = self.memory.sample()\n",
    "                self.learn(samples, GAMMA)\n",
    "    \n",
    "    def act(self, state):\n",
    "        state = torch.from_numpy(state).float()\n",
    "        with torch.no_grad():\n",
    "            action_values = self.actor_local(state)\n",
    "        \n",
    "        action_values += (torch.tensor(self.noise.sample()).float())\n",
    "        return np.clip(action_values, -1, 1)    \n",
    "            \n",
    "    def learn(self, samples, gamma):\n",
    "        states, actions, rewards, next_states, dones = samples\n",
    "        rewards = rewards.unsqueeze(1)\n",
    "        dones = dones.unsqueeze(1)\n",
    "        q_values_next_states = self.critic_target.forward(next_states, self.actor_target(next_states))\n",
    "        targets = rewards + (gamma * (q_values_next_states) *  (1 - dones))\n",
    "        predictions = self.critic_local.forward(states, actions)\n",
    "\n",
    "        loss = F.mse_loss(predictions, targets)\n",
    "        self.critic_optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.critic_optimizer.step()\n",
    "        \n",
    "        actor_losses = self.critic_local.forward(states, self.actor_local(states))\n",
    "        actor_loss = - actor_losses.mean()\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "\n",
    "        self.soft_update(self.critic_local, self.critic_target, TAU)  \n",
    "        self.soft_update(self.actor_local, self.actor_target, TAU)  \n",
    " \n",
    "    def soft_update(self, local_model, target_model, tau):\n",
    "        \"\"\"Soft update model parameters.\n",
    "        θ_target = τ*θ_local + (1 - τ)*θ_target\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            local_model (PyTorch model): weights will be copied from\n",
    "            target_model (PyTorch model): weights will be copied to\n",
    "            tau (float): interpolation parameter \n",
    "        \"\"\"\n",
    "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "            target_param.data.copy_(tau*local_param.data + (1.0-tau)*target_param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot scores over time with box showing parameter set\n",
    "def plot_scores(scores, last_100_scores_rolling_means, episode_count,\n",
    "                buffer_size, batch_size, lr_actor, lr_critic, tau,\n",
    "                actor, critic):\n",
    "    fig, ax = plt.subplots(figsize=(20, 10))\n",
    "    textstr = 'max(last_100_scores_means): {}\\nepisode_count: {}\\nbuffer_size: {}\\nbatch_size: {}\\nlr_actor: {}\\nlr_critic: {}\\ntau: {}\\nactor: {}\\ncritic: {} '\n",
    "    textstr = textstr.format(round(np.max(last_100_scores_rolling_means), 2),\n",
    "                             episode_count, buffer_size, batch_size, \n",
    "                             lr_actor, lr_critic, tau, actor, critic)\n",
    "    ax.plot(np.arange(len(scores)), scores, 'o', label='Single episode score')\n",
    "    ax.plot(np.arange(len(last_100_scores_rolling_means)),\n",
    "            last_100_scores_rolling_means,\n",
    "            label='Last 100 scores rolling mean')\n",
    "    props = dict(boxstyle='round', facecolor='wheat', alpha=0.5)\n",
    "    ax.text(0.05, 0.95, textstr, transform=ax.transAxes,\n",
    "            verticalalignment='top', bbox=props)\n",
    "    ax.legend(loc='upper right')\n",
    "    ax.set_title('Score over number of episodes')\n",
    "    ax.set_xlabel('Episode number')\n",
    "    ax.set_ylabel('Score')\n",
    "    filename = 'scores_over_episodes.png'\n",
    "    plt.savefig(filename)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run MADDPG algorithm\n",
    "def run_maddpg(n_episodes=8000, max_t=1000):\n",
    "    env_info = env.reset(train_mode=True)[brain_name]\n",
    "    agent = Agent(0)\n",
    "    scores = []\n",
    "    last_100_scores = deque(maxlen=100)\n",
    "    last_100_scores_rolling_means = []\n",
    "    \n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        env_info = env.reset(train_mode=True)[brain_name]\n",
    "        states = env_info.vector_observations\n",
    "        score = 0       \n",
    "        while True:\n",
    "            actions = []\n",
    "            for state in states:\n",
    "                action = agent.act(state).numpy()\n",
    "                actions.append(action)     \n",
    "            env_info = env.step(actions)[brain_name]\n",
    "            next_states = env_info.vector_observations\n",
    "            rewards = env_info.rewards\n",
    "            dones = env_info.local_done\n",
    "            experiences = zip(states, actions, rewards, next_states, dones)\n",
    "            for (state,action, reward, next_state, done) in experiences:\n",
    "                agent.step(state, action, reward, next_state, done)\n",
    "            states = next_states\n",
    "            score += np.max(rewards)\n",
    "            if done:\n",
    "                break\n",
    "        last_100_scores.append(score)\n",
    "        scores.append(score)\n",
    "        last_100_scores_rolling_means.append(np.mean(last_100_scores))\n",
    "        plot_scores(scores, last_100_scores_rolling_means, i_episode, BUFFER_SIZE, BATCH_SIZE, ACTOR_LR, CRITIC_LR, TAU,\n",
    "                    agent.actor_local, agent.critic_local)\n",
    "\n",
    "        if i_episode % 100 == 0:\n",
    "            print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(last_100_scores)))\n",
    "        is_problem_solved = np.mean(last_100_scores) >= 0.5\n",
    "        if is_problem_solved:\n",
    "            solved_str = '\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'\n",
    "            print(solved_str.format(i_episode, np.mean(last_100_scores)))\n",
    "            torch.save(agent.actor_local.state_dict(), 'actor_weights.pth')\n",
    "            torch.save(agent.critic_local.state_dict(),'critic_weights.pth' )\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# track start time\n",
    "start_time = time.time()\n",
    "local_time = time.localtime(int(start_time))\n",
    "local_time_str = time.strftime(\"%Y-%m-%d %H:%M:%S\", local_time)\n",
    "print('\\nStarted training on '+local_time_str)\n",
    "\n",
    "# train DDPG agent\n",
    "run_maddpg()\n",
    "\n",
    "# track end time and print training time\n",
    "end_time = time.time()\n",
    "training_time = round((end_time - start_time) / 60, 1)\n",
    "local_time = time.localtime(int(start_time))\n",
    "local_time_str = time.strftime(\"%Y-%m-%d %H:%M:%S\", local_time)\n",
    "print('\\nFinished training on '+local_time_str)\n",
    "print('\\nTotal training time: {} minutes'.format(training_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
